% XeLaTeX can use any Mac OS X font. See the setromanfont command below.
% Input to XeLaTeX is full Unicode, so Unicode characters can be typed directly into the source.

% The next lines tell TeXShop to typeset with xelatex, and to open and save the source with Unicode encoding.

%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[12pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathabx}
\usepackage{makecell}
\usepackage{cite}

% Will Robertson's fontspec.sty can be used to simplify font choices.
% To experiment, open /Applications/Font Book to examine the fonts provided on Mac OS X,
% and change "Hoefler Text" to any of these choices.

\usepackage{fontspec,xltxtra,xunicode}
\defaultfontfeatures{Mapping=tex-text}
\setromanfont[Mapping=tex-text]{Times New Roman}
\setsansfont[Scale=MatchLowercase,Mapping=tex-text]{Gill Sans}
\setmonofont[Scale=MatchLowercase]{Andale Mono}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\matrixSymbol}[1]{\mathbf{#1}}
\newcommand{\expectation}{\mathbb{E}}
\newcommand{\vecop}[1]{\mathrm{vec}({#1})}
\newcommand{\transpose}[1]{{#1}^{\intercal}}
\newcommand{\normtwo}[1]{{\lVert {#1} \rVert}_{2}}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{common}{Common}
\newtheorem{corollary}{Corollary}
\newtheorem*{remark}{Remark}

\allowdisplaybreaks[1]

\pagenumbering{gobble}

\title{Supplementary Notes}
%\author{Yang Ding}
\date{}                                            % Activate to display a given date or no date

\begin{document}
\maketitle

\section{Definitions}

%% Prob -> P

\begin{common}
Throughout this text, matrices are denoted by boldface capital letters, e.g., $\matrixSymbol{A} = (a_{i, j})$, and vectors are denoted by boldface lowercase letters, e.g., $\vect{a}=(a_{i})$. All vectors are assumed to be column vectors. All subscripts are one-based.

In the proof we need to describe the submatrix of a given matrix. We use $\matrixSymbol{A}[i_1:i_2, j_1:j_2], i_1 \le i_2 , j_1 \le j_2 $ to describe the submatrix of $\matrixSymbol{A}$ generated by extracting rows $i_1 , i_1 +1 , \cdots, i_2$ and columns $j_1, j_1 + 1, \cdots, j_2$.

For a matrix $\matrixSymbol{A}$ and a scalar $b$, we assume $\matrixSymbol{A} + b$ is an element-wise addition, i.e. $(\matrixSymbol{A} + b)_{i, j} := a_{i, j} + b$.

The vector $\vect{1}_{a}$ is a column vector of length $a$ filled with 1.
\end{common}

\begin{common}
We use $\matrixSymbol{W}$ as the matrix representation of an arbitrary kernel. It always has 4 rows, and the number of its columns is denoted by $L (L > 0)$.
\end{common}

\begin{common}
We use $\matrixSymbol{X}$ as the matrix representation of an arbitrary input sequence. Similar to kernels, it always has 4 rows. The number of its columns is denoted by $N$. In the current problem setting, we only consider cases where input sequences are not shorter than kernels, and thus we assume that $N \ge L$.

We use one-hot encoding to represent an arbitrary input sequence using a matrix. Specifically, for any sequence $( s_1, \cdots, s_N ) $ with $ s_j \in \{\text{A, C, G, T}\}, \forall j \in 1, \cdots, N$, its matrix representation $\matrixSymbol{X}$ could be written explicitly in the following form:

\begin{align}
x_{1j} & := \begin{cases} 1 & s_j=\text{A} \\ 0 & \text{otherwise} \\ \end{cases} \notag \\
x_{2j} & := \begin{cases} 1 & s_j=\text{C} \\ 0 & \text{otherwise} \\ \end{cases} \\
x_{3j} & := \begin{cases} 1 & s_j=\text{G} \\ 0 & \text{otherwise} \\ \end{cases} \notag \\
x_{4j} & := \begin{cases} 1 & s_j=\text{T} \\ 0 & \text{otherwise} \\ \end{cases} \notag
\end{align}

\end{common}


\begin{common}
For the sake of convenience we assume that, if $a_1 > a_2$, then the summation $\sum_{i=a_1}^{a_2}{ \left( x_i \right)} = 0$.
\end{common}


\begin{definition}
We define $ f(j|\matrixSymbol{X}) : \{1, \cdots, N\} \rightarrow \{1, 2, 3, 4\}$ to describe the nucleotide identity of the $j$-th nucleotide of the sequence that is represented by the matrix $\matrixSymbol{X}$. Specifically, we have

\begin{align}
 f(j|\matrixSymbol{X}) := i \text{  satisfying  } x_{i,j} = 1 
\end{align}

Since each column of $\matrixSymbol{X}$ has one and only one element being $1$, this function is well-defined. 


% We could use indicator function ( $\mathbf{1}_{\text{statement}} = \begin{cases} 1 & \text{statement is true} \\ 0 & \text{statement is false} \\ \end{cases}$ ) to simplify the notation:

% \begin{align}
%  f(j|\matrixSymbol{X}) = \sum_{i=1}^{4} \left(i \mathbf{1}_{x_{ij} = 1} \right)
% \end{align}

\end{definition}


\begin{definition}
We define $\matrixSymbol{P}(\matrixSymbol{W}, b) : \mathbb{R}^{4 \times L} \rightarrow \mathbb{R}^{4 \times L} $ as the matrix representation of the PWM of the sequence profile transformed from the kernel $\matrixSymbol{W}$ with base for logarithm $b >0$. Details of the transformation are described explicitly in the main text.
\end{definition}

\begin{definition}
We define $\matrixSymbol{X} \Asterisk \matrixSymbol{W}$ as the discrete convolution with matrix $\matrixSymbol{W}$ on matrix $\matrixSymbol{X}$. Specifically, we have:

\begin{align}
  \left( \matrixSymbol{X} \Asterisk \matrixSymbol{W} \right)_{j} := \sum_{i=1}^{4}{\sum_{j'=1}^{L}{x_{i, j+j'-1}w_{i, L-j'+1}}} 
\end{align}
\end{definition}

\begin{definition}
We define $\mathrm{Prob}(\matrixSymbol{X}|\matrixSymbol{P}(\matrixSymbol{W}, b))$ as the probability that the sequence represented by matrix $\matrixSymbol{X}$ is generated from the sequence profile whose PWM is represented by the matrix $\matrixSymbol{P}(\matrixSymbol{W}, b)$ defined above.
\end{definition}

\begin{definition}
We define the elementwise exponential ${b}^{\matrixSymbol{A}}$ as:

\begin{align}
\left( {b}^{\matrixSymbol{A}} \right)_{i, j} := {b}^{\left( \matrixSymbol{A}_{i, j} \right)}
\end{align}
\end{definition}

\begin{definition}
We define $\mathrm{threshold}(\vect{x}, z)$ as:

\begin{align}
(\mathrm{threshold}(\vect{x}, z))_{i} := \begin{cases}x_i, & \text{if } x_i > z \\ 0, & \text{otherwise} \end{cases}
\end{align}

Specifically, we define $(\vect{x})_{+} := \mathrm{threshold}(\vect{x}, 0)$.
\end{definition}




\section{Theorem 1 and its proof}

\begin{theorem}
$\forall \matrixSymbol{W}, b > 0 $, transform them into the PWM $\matrixSymbol{P}=\matrixSymbol{P}(\matrixSymbol{W}, b)$ according to the algorithm described in the main text of this paper. Then, $\forall \matrixSymbol{X}$, we have: 
\begin{align}
 (\matrixSymbol{X} \Asterisk \matrixSymbol{W})_j = \log_b{\mathrm{Prob}(\matrixSymbol{X}[1:4, j:(j+L-1)]|\matrixSymbol{P}(\matrixSymbol{W}, b))} + d(\matrixSymbol{W}, b)
\end{align}
where $d(\matrixSymbol{W}, b)$ is a constant that does not depend on $\matrixSymbol{X}$, but entirely on $\matrixSymbol{W}$ and $b$.
\end{theorem}

\begin{proof}
By definition of PWM, the probability that a sequence is generated from a PWM is the product of generating probability of ``the nucleotide at each position in that sequence'' at the same position in this PWM. Mathematically, this is:
\begin{align}
  & \mathrm{Prob}(\matrixSymbol{X}[1:4, j:(j+L-1)]|\matrixSymbol{P}(\matrixSymbol{W}, b)) \notag \\
= & \prod_{j'=1}^{L}{\left( \matrixSymbol{P}(\matrixSymbol{W}, b)[f(j'|\matrixSymbol{X}[1:4, j:(j+L-1)]), j']\right)}
\end{align}

Taking logarithm with base $b$ of both sides gives the following result:
\begin{align} 
  & \log_{b}{\mathrm{Prob}(\matrixSymbol{X}[1:4, j:(j+L-1)]|\matrixSymbol{P}(\matrixSymbol{W}, b))} \notag \\
= &  \log_{b}{\prod_{j'=1}^{L}{\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j'|\matrixSymbol{X}[1:4, j:(j+L-1)]), j'}\right)}} \\
%  & \text{(decomposing the logarithm)} \\
= & \sum_{j'=1}^{L}{\log_{b}{\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j'|\matrixSymbol{X}[1:4, j:(j+L-1)]), j'}\right)}} \notag \\
%% TODO emphasize that the relative coordinate system changes
%  & \text{(switching the coordinates of $f(\cdot)$ from $\matrixSymbol{X}[1:4, j:(j+L-1)]$ to the whole $\matrixSymbol{X}$)} \\
= & \sum_{j'=1}^{L}{\log_{b}{\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X}), j'}\right)}} \notag
\end{align}

On the other hand, for the convolution we have:

\begin{align}
  & \left( \matrixSymbol{X} \Asterisk \matrixSymbol{W} \right)_j  \notag \\
%  & \text{(by definition of convolution)} \\
= & \sum_{i=1}^{4}{\sum_{j'=1}^{L}{ \left( x_{i, j+j'-1}w_{i, L-j'+1} \right) } } \\
  & \text{Replacing $\matrixSymbol{W}$ with its flipped version $\matrixSymbol{W'}$ which has $w'_{i, j} := w_{i, L-j+1}$, we have} \notag \\
= & \sum_{i=1}^{4}{\sum_{j'=1}^{L}{ \left( x_{i, j+j'-1}w'_{i, j'} \right) }  } \\
%  & \text{(using the fact that $\matrixSymbol{X}$ has one 1 and three 0's in each column)} \\
= & \sum_{j'=1}^{L}{\left( w'_{f(j + j' - 1|\matrixSymbol{X}), j'} \right) } \notag \\
  & \text{Replacing $\matrixSymbol{W'}$ with its exponentiated version $\matrixSymbol{C} := b^{\matrixSymbol{W'}}$, we have} \notag \\
= & \sum_{j'=1}^{L}{ \log_{b}\left( c_{f(j + j' - 1|\matrixSymbol{X}), j'} \right) } \\
  & \text{Replacing $\matrixSymbol{C}$ with $\matrixSymbol{P}(\matrixSymbol{W}, b)$ where $ (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j}  := \frac{c_{i, j}}{\sum_{i'=1}^{4}{c_{i', j}}} $,  we have} \notag \\
= & \sum_{j'=1}^{L}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X}), j'} \sum_{i=1}^{4}{c_{i', j'}} \right) } \\
%  & \text{Decomposing the logarithm gives} \\
= & \sum_{j'=1}^{L}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X}), j'} \right) } + \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{c_{i', j'}} \right) } \notag \\
  & \text{Inserting the probability formula for PWM derived above gives us} \notag  \\ 
= & \log_{b}{\mathrm{Prob}(\matrixSymbol{X}[1:4, j:(j+L-1)]|\matrixSymbol{P}(\matrixSymbol{W}, b))} + \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{c_{i', j'}} \right) }  \\
  & \text{Noting that the latter term depends entirely on $\matrixSymbol{W}$ and $b$, we have} \notag \\
= & \log_{b}{\mathrm{Prob}(\matrixSymbol{X}[1:4, j:(j+L-1)]|\matrixSymbol{P}(\matrixSymbol{W}, b))} + d(\matrixSymbol{W}, b) 
\end{align}

which concludes the proof.
\end{proof}



\section{Theorem 2 and its proof}

\begin{theorem}
Assume that the given deep learning framework has its output function $g(\matrixSymbol{X}|\Theta)$ (i.e. parameterized by the parameter set $\Theta$) of the following form: 
\begin{align}
g(\matrixSymbol{X}|\Theta) := u \left( h_1 \left( \matrixSymbol{X} \Asterisk \matrixSymbol{W_{1}}  | \theta_1 \right), h_2 \left( \matrixSymbol{X} \Asterisk \matrixSymbol{W_{2}}  | \theta_2 \right), \cdots, h_k \left( \matrixSymbol{X} \Asterisk \matrixSymbol{W_{k}}  | \theta_k \right) | \gamma \right)
\end{align}
where $\matrixSymbol{W_i},  \forall i=1, \cdots, k $ are the kernels (and thus part of the parameter set of this model) used for convolution, each $h_i(\cdot|\theta_i ), \forall i=1, \cdots, k $ is a function of one of the following forms parameterized by $\theta_i$:
\begin{description}
\item[linear] \begin{equation}h_i(\vect{x}|\theta_i ) = \matrixSymbol{A}_{\theta_i}\vect{x} + \vect{b}_{\theta_i}\end{equation}
\item[max-linear] \begin{equation}h_i(\vect{x}|\theta_i ) = a_{\theta_i}\max{\vect{x}} + b_{\theta_i} \end{equation}
\item[thresholding-max-linear]  \begin{equation}h_i(\vect{x}|\theta_i ) = a_{\theta_i}\max{(\mathrm{threshold}(\vect{x}, z_{\theta_i}))} + b_{\theta_i} \end{equation}
\end{description}
, and $u(\cdot | \gamma)$ an arbitrary function whose paramter is $\gamma$. Then for each parameter set $\Theta^{*}$, there are uncountably infinite parameter sets each of which (denoted as $\Theta'$) satisfying $g(\matrixSymbol{X}|\Theta') = g(\matrixSymbol{X}|\Theta^{*})$ when $\matrixSymbol{X}$ is fixed.
\end{theorem}

\begin{proof}

The idea of this proof is: for each scaling/shifting of kernels, we could always find a way to modify the parameters of $h_i(\vect{x}|\theta_i)$ (i.e. $\theta_i$) to make the output of $h_i(\cdot)$ unchanged.

Assume all kernels are of length $L$ and the input sequence $X$ is of length $N$. Write $\Theta^{*}$ as $\Theta^{*} = \{ \matrixSymbol{W_1^*}, \cdots, \matrixSymbol{W_k^*}, \theta_1^*, \cdots, \theta_k^*, \gamma^* \} $. Then, due to the identities listed below, $ \forall r \in \mathbb{R}^{+}, t \in \mathbb{R}$, we could always construct $\theta_i'(r, t)$ such that  $h_i \left( \matrixSymbol{X} \Asterisk \matrixSymbol{W_{i}^*}  | \theta_i^* \right) = h_i \left( ( \matrixSymbol{X} \Asterisk (r \matrixSymbol{W_{i}^*} + t) )  | \theta_i'(r, t) \right)$.
\begin{description}
\item[linear] \begin{align} 
  & h_i \left( \matrixSymbol{X} \Asterisk \matrixSymbol{W_{i}^*}  | \theta_i^* \right) \notag \\
= & \matrixSymbol{A}_{\theta_i} (\matrixSymbol{X} \Asterisk \matrixSymbol{W_{i}^*} ) + \vect{b}_{\theta_i} \\
= & \frac{1}{r}\matrixSymbol{A}_{\theta_i} (\matrixSymbol{X} \Asterisk (r \matrixSymbol{W_{i}^*} + t)) + (\vect{b}_{\theta_i} - \frac{tL}{r}\matrixSymbol{A}_{\theta_i}\vect{1}_{N-L+1}) \notag  \end{align}
\item[max-linear] \begin{align} 
  & h_i \left( \matrixSymbol{X} \Asterisk \matrixSymbol{W_{i}^*}  | \theta_i^* \right) \notag \\
= & a_{\theta_i}\max{( \matrixSymbol{X} \Asterisk \matrixSymbol{W_{i}^*} )} + b_{\theta_i} \\ 
= & \frac{a_{\theta_i}}{r}\max{( \matrixSymbol{X} \Asterisk (r \matrixSymbol{W_{i}^*} + t))} + (b_{\theta_i} - \frac{ta_{\theta_i}}{r}) \notag \end{align}
\item[thresholding-max-linear] \begin{align} 
  & h_i \left( \matrixSymbol{X} \Asterisk \matrixSymbol{W_{i}^*}  | \theta_i^* \right) \notag \\
= & a_{\theta_i}\max{(\mathrm{threshold}( \matrixSymbol{X} \Asterisk \matrixSymbol{W_{i}^*}, z_{\theta_i} ))} + b_{\theta_i} \\ 
= & \frac{a_{\theta_i}}{r}\max{( \mathrm{threshold}( \matrixSymbol{X} \Asterisk (r\matrixSymbol{W_{i}^*}+t), rz_{\theta_i}+t )  )} + ( b_{\theta_i} -  \frac{ta_{\theta_i}}{r} ) \notag \end{align}
\end{description}

Specifically, the construction of $\theta_i'(r, t)$ is:
\begin{description}
\item[linear] \begin{equation} \matrixSymbol{A}_{\theta_i'} := \frac{1}{r}\matrixSymbol{A}_{\theta_i}, \vect{b}_{\theta_i'} := (\vect{b}_{\theta_i} - \frac{t}{r}\matrixSymbol{A}_{\theta_i}) \end{equation}
\item[max-linear] \begin{equation} a_{\theta_i'} := \frac{a_{\theta_i}}{r}, b_{\theta_i'} := (b_{\theta_i} - \frac{ta_{\theta_i}}{r}) \end{equation}
\item[thresholding-max-linear] \begin{equation} z_{\theta_i'} := rz_{\theta_i}+t, a_{\theta_i'} := \frac{a_{\theta_i}}{r}, b_{\theta_i'} := (b_{\theta_i} - \frac{ta_{\theta_i}}{r}) \end{equation}
\end{description}

Therefore, if we define $\matrixSymbol{W_{i}'} := r\matrixSymbol{W_{i}^*}$, then $\Theta_i' := \{ \matrixSymbol{W_1'}, \cdots, \matrixSymbol{W_k'}, \theta_1', \cdots, \theta_k', \gamma^* \}$ satisfies the identity $g(\matrixSymbol{X}|\Theta') = g(\matrixSymbol{X}|\Theta^{*})$.

This implies that for each solution and each pair of $(r, t)$ that changes the i-th kernel $\matrixSymbol{W_i}$, we could always find some parameters for $h_i(\cdot)$ to build up a new $\Theta_i'$ that keep the output of $h_i(\cdot)$, and thus the output of $g(\cdot)$, unchanged. Since the number of all possible pairs of $(r, t)$ (including the case $(r, t=0)$) is uncountably infinite, we could create uncountably infinite number of such $\Theta_i'$.

\end{proof}

When all $h_i(\cdot)$ are thresholding-max-linear with $z_{\theta_i}=0$, $g(\cdot)$ is essentially a convolution layer with ReLU activation, followed by a global max-pooling layer, then by other arbitrary layers. If the thresholds ($z_{\theta_i}$) are allowed to change, then Theorem 2 could be applied directly; if not, Theorem 2 could still be applied by enforcing that $t$ must be 0 no matter what $r$ is chosen.

Also, note that other activation functions and other model structures might also make Theorem 2 hold, as long as a similar construction is given. For example, 
\begin{enumerate}
\item A global average pooling could be linear by requiring that the input sequences have the same length, and thus Theorem 2 could be applied to a CNN framework where convolution is followed by a global average pooling.
\item A local max-pooling $h(\vect{c})$ is equivalent to the concatenation of the output of a series of $h_i(\vect{c}) := \max(l(\vect{c}))$ , where each $h_i(\vect{c})$ is applied to a separate window from the local pooling, and $l(\vect{c})$ is a linear transformation that adds a very negative value to all elements in $\vect{c}$ out of the window, and 0 to elements in the window. Since Theorem 2 ensures that each $h_i(\vect{c})$ could have their output unchanged regardless of how the kernel changes, it could also be applied to $h(\vect{c})$.
\item Similarly, Theorem 2 also holds for local average-pooling.
\item Since the $h(\cdot)$'s are independent of each other, Theorem 2 also holds for CNN frameworks where multiple layers are connected to the convolution output at the same time (e.g., by connecting to the convolution a global max-pooling layer and also a global average-pooling layer).
\end{enumerate}
Therefore, Theorem 2 could be accessed by a variety of CNN frameworks that have been used for handling DNA/RNA sequences (e.g., \cite{alipanahi_predicting_2015, zhou_predicting_2015, quang_danq:_2016} ).

\section{Corollaries 1 and 2 and their deduction}

\begin{corollary}
For each sequence profile, there are uncountably infinite other sequence profiles that classify sequences in the same way as this sequence profile does when used in deep learning frameworks with output function $g(\cdot)$ as described in Theorem 2.
\end{corollary}


\begin{proof}
In the following deduction, only models whose structure satisfying those criteria in Theorem 2 will be considered. In addition, we will only prove the special case where there is only one kernel in the convolution layer applied to the input sequences directly, as the proof could obviously extends to models with multiple kernels.

According to the transformation the resulting PWM $\matrixSymbol{P}(\matrixSymbol{W}, b)$ satisfies $(\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j'} = \frac{c_{i, j'}} {\sum_{i'=1}^{4}{ c_{i', j'}}} = \frac{b^{w'_{i, j'}}} {\sum_{i'=1}^{4}{b^{w'_{i', j'}}}} = \frac{b^{w_{i, L-j'+1}}} {\sum_{i'=1}^{4}{b^{w_{i', L-j'+1}}}}$.

As for its performance on sequence regression/classification, note that for any kernel $\matrixSymbol{W}$, we have the identity in Theorem 1
\begin{align}
(\matrixSymbol{X} \Asterisk \matrixSymbol{W})_j & = \sum_{j'=1}^{L}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X}), j'} \right) } + \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{c_{i', j'}} \right)} \\ 
& = \sum_{j'=1}^{L}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X}), j'} \right) } + \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{b^{w'_{i', j'}}} \right) } \notag \\
& = \sum_{j'=1}^{L}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X}), j'} \right) } + \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{b^{w_{i', L-j'+1}}} \right) } \notag \\
& = \sum_{j'=1}^{L}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X}), j'} \right) } + \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{b^{w_{i', j'}}} \right) } \notag 
\end{align}


Taking advantage of the one-hot encoding nature of $\matrixSymbol{X}$, we have 
\begin{align}
(\matrixSymbol{X} \Asterisk (\matrixSymbol{W} - \frac{1}{L}\sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{b^{w_{i', j'}}} \right)}))_j = \sum_{j'=1}^{L}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X}), j'} \right) }
\end{align}

By Theorem 2, $\matrixSymbol{W}$ and $(\matrixSymbol{W} - \frac{1}{L}\sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{b^{w_{i', j'}}} \right)})$ (when regarded as a new kernel) could regress/classify sequences exactly the same. Since the latter is identical to the log-likelihood computed by $\matrixSymbol{P}(\matrixSymbol{W}, b)$, we could say that $\matrixSymbol{W}$ and  $\matrixSymbol{P}(\matrixSymbol{W}, b)$ could also regress/classify sequences exactly the same. 



Now let's check what will happen to the PWM itself and its performance on sequence regression/classification, if the base for calculating the log-likelihood is changed. 

Without loss of generality, if we replace $b$ with $b^{r} (r>0, r \neq 1)$, the new PWM $\matrixSymbol{P}(\matrixSymbol{W}, b^{r})$ satisfies $(\matrixSymbol{P}(\matrixSymbol{W}, b^{r}))_{i, j'} = \frac{(b^{r})^{w_{i, L-j'+1}}} {\sum_{i'=1}^{4}{(b^{r})^{w_{i', L-j'+1}}}}$. Then it can be shown that changing the base will definitely change the resulting PWM, unless each column of the kernel is filled with identical elements (which is rarely possible). Specifically, suppose there is some $r>0, r \neq 1$ that will not change the resulting PWM for some kernel $\matrixSymbol{W}$: 

\begin{align}
(\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j'} = (\matrixSymbol{P}(\matrixSymbol{W}, b^{r}))_{i, j'}
\end{align}
i.e., 
\begin{align}
\frac{b^{w_{i, L-j'+1}}} {\sum_{i'=1}^{4}{b^{w_{i', L-j'+1}}}} = \frac{(b^{r})^{w_{i, L-j'+1}}} {\sum_{i'=1}^{4}{(b^{r})^{w_{i', L-j'+1}}}}
\end{align}
then we have
\begin{align}
\frac{b^{w_{i, L-j'+1}}} {(b^{r})^{w_{i, L-j'+1}}} = \frac{\sum_{i'=1}^{4}{b^{w_{i', L-j'+1}}}} {\sum_{i'=1}^{4}{(b^{r})^{w_{i', L-j'+1}}}}
\end{align}
since the RHS does not depend on $i$, we can, for example,  replace $i$ with 1 and again with 2, and have
\begin{align}
\frac{b^{w_{1, L-j'+1}}} {(b^{r})^{w_{1, L-j'+1}}} = \frac{b^{w_{2, L-j'+1}}} {(b^{r})^{w_{2, L-j'+1}}}
\end{align}
therefore
\begin{align}
\frac{b^{w_{1, L-j'+1}}} {b^{w_{2, L-j'+1}}} = \frac{(b^{r})^{w_{1, L-j'+1}}} {(b^{r})^{w_{2, L-j'+1}}} = \left( \frac{b^{w_{1, L-j'+1}}} {b^{w_{2, L-j'+1}}} \right) ^r
\end{align}
because $b > 0$, $\frac{b^{w_{1, L-j'+1}}} {b^{w_{2, L-j'+1}}} > 0 $, and we have
\begin{align}
1 = \left( \frac{b^{w_{1, L-j'+1}}} {b^{w_{2, L-j'+1}}} \right) ^{r-1}
\end{align}
now we use the assumption that $r-1 \neq 0$, and because $\frac{b^{w_{1, L-j'+1}}} {b^{w_{2, L-j'+1}}} > 0$, we must have 
\begin{align}
\frac{b^{w_{1, L-j'+1}}} {b^{w_{2, L-j'+1}}} = 1
\end{align}
i.e., 
\begin{align}
w_{1, L-j'+1} = w_{2, L-j'+1}
\end{align}
but note that the deduction above does not depend on specific values of $i$, and thus we have
\begin{align}
w_{1, L-j'+1} = w_{2, L-j'+1} = w_{3, L-j'+1} = w_{4, L-j'+1}
\end{align}
which concludes the proof. Note that when $r \neq 1$ but changing the base does not change the resulting PWM, the resulting PWM must be a matrix filled with 0.25. Such kernel (and the resulting PWM) is impossible to distinguish any two sequences from each other, and thus is very unlikely to be from a trained model that regress/classify sequences well.

However, a change in the resulting PWM is not accompanied with a change in the performance. Note that the identity below:

\begin{align}
(\matrixSymbol{X} \Asterisk \matrixSymbol{W})_j =  \sum_{j'=1}^{L}{ \log_{b^{r}}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b^{r} ))_{f(j + j' - 1|\matrixSymbol{X}), j'} \right) } + \sum_{j'=1}^{L}{\log_{b^{r}} \left( \sum_{i'=1}^{4}{(b^{r})^{w_{i', j'}} }\right) } 
\end{align}

could be rearranged in a way similar to the previous one:

\begin{align}
  & (\matrixSymbol{X} \Asterisk (r\matrixSymbol{W} -\frac{1}{L}\sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{(b^{r})^{w_{i', j'}} }\right) }  ) )_j \notag \\
= & \sum_{j'=1}^{L}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b^{r} ))_{f(j + j' - 1|\matrixSymbol{X}), j'} \right) } 
\end{align}

Again by Theorem 2, $\matrixSymbol{W}$ and $(r\matrixSymbol{W} -\frac{1}{L}\sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{(b^{r})^{w_{i', j'}} }\right) }  )$ (when regarded as a new kernel), and thus $\matrixSymbol{P}(\matrixSymbol{W}, b^{r} )$, could regress/classify sequences exactly the same.

Combining all the equivalence relationships deduced above, we arrived at the conclusion: $\matrixSymbol{P}(\matrixSymbol{W}, b)$ and $\matrixSymbol{P}(\matrixSymbol{W}, b^{r} )$ could also regress/classify sequences exactly the same. Since $r$ could be any positive real number in $\mathbb{R}$, for each PWM $\matrixSymbol{P}(\matrixSymbol{W}, b)$ we could always construct uncountably infinite $\matrixSymbol{P}(\matrixSymbol{W}, b^{r} )$'s that has the same performance on regression/classification, thus concluding the proof.
\end{proof}

% Note that $(\matrixSymbol{P}(\matrixSymbol{W}, b^{r}))_{i, j'} = \frac{(b^{r})^{w_{i, j'}}} {\sum_{i=1}^{4}{(b^{r})^{w_{i, j'}}}} = \frac{(b)^{rw_{i, j'}}} {\sum_{i=1}^{4}{(b)^{rw_{i, j'}}}} = \frac{(b)^{rw_{i, j'}+t}} {\sum_{i=1}^{4}{(b)^{rw_{i, j'}+t}}} = (\matrixSymbol{P}(r\matrixSymbol{W}+t, b))_{i, j'}$ for any $t$, which means that $\matrixSymbol{P}(\matrixSymbol{W}, b^{r} )$ and $\matrixSymbol{P}(r\matrixSymbol{W}+t, b)$ are identical.

% However, note that when the base is $b^{r}$, the identity could be rearranged into the following form:
% \begin{align}
% (\matrixSymbol{X} \Asterisk (r\matrixSymbol{W} + t))_j =  \sum_{j'=1}^{L}{ \log_{b}\left( (\matrixSymbol{P}(r\matrixSymbol{W}+t, b ))_{f(j + j' - 1|\matrixSymbol{X}), j'} \right) } + \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i=1}^{4}{(b)^{w_{i, j'}} }\right) } + tL 
% \end{align}
% with $t = \frac{1}{L} (\sum_{j'=1}^{L}{\log_{b} \left( \sum_{i=1}^{4}{(b)^{rw_{i, j'}} }\right) } - \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i=1}^{4}{(b)^{w_{i, j'}} }\right) })$.

% Therefore, by Theorem 2, $\matrixSymbol{W}$ and $r\matrixSymbol{W}+t$ could have identical performance on sequence classifcation/regression, which means that $\matrixSymbol{P}(\matrixSymbol{W}, b )$ and $\matrixSymbol{P}(\matrixSymbol{W}, b^{r} ) = \matrixSymbol{P}(r\matrixSymbol{W}+t, b)$ also have identical performance on the same problem. Therefore, we have the following corollary:


This raises the concern that additional constraints must be applied when selecting the optimal kernels (and thus sequence profiles) based on the trained model. A popular choice would be to use maximum likelihood estimation to determine the base $b$, which will be introduced in the next section.

On the other hand, it doesn’t escape from our attention that this corollary also suggests that the classical sequence logo (e.g., the one used in Deepbind's paper \cite{alipanahi_predicting_2015} ) does not fit CNN-oriented profile visualization.

\begin{corollary}
$\forall \matrixSymbol{W}^{**}, \matrixSymbol{W}^{*} $, we have: $\forall b (b > 1), \matrixSymbol{P}(\matrixSymbol{W}^{**}, b) = \matrixSymbol{P}(\matrixSymbol{W}^{*}, b)$  if and only if $\forall i \in \{1, \cdots, 4\}, j \in \{1, \cdots, L\}, w^{**}_{i,j} = w^{*}_{i, j} + t_{j} $, where $t_{j} \in \mathbb{R}$ is independent of $\matrixSymbol{W}^{**}$ and $ \matrixSymbol{W}^{*} $ .
\end{corollary}



\begin{proof}

We prove the ``if'' direction and the ``only-if'' direction separately.

\begin{description}
\item[The ``if'' direction] This is a natural result of the transformation. Specifically, we have

\begin{align}
 (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j}  = \frac{c_{i, j}}{\sum_{i'=1}^{4}{c_{i', j}}} = \frac{b^{w'_{i, j}}}{\sum_{i'=1}^{4}{b^{w'_{i', j}}}} = \frac{b^{w_{i, L-j+1}}}{\sum_{i'=1}^{4}{b^{w_{i', L-j+1}}}}
\end{align}

And therefore

\begin{align}
  & (\matrixSymbol{P}(\matrixSymbol{W}^{**}, b))_{i, j} \notag \\
= & \frac{b^{w^{**}_{i, L-j+1}}}{\sum_{i'=1}^{4}{b^{w^{**}_{i', L-j+1}}}} = \frac{b^{w^{*}_{i, L-j+1} + t_{j}}}{\sum_{i'=1}^{4}{b^{w^{*}_{i', L-j+1} + t_{j}}}} = \frac{b^{w^{*}_{i, L-j+1}}}{\sum_{i'=1}^{4}{b^{w^{*}_{i', L-j+1}}}} \\
= & (\matrixSymbol{P}(\matrixSymbol{W}^{*}, b))_{i, j} \notag
\end{align}

which concludes the proof.

\item[The ``only-if'' direction] Fix $j \in \{1, \cdots, L\}$. As in the proof of the ``if'' direction, we already have for any $\matrixSymbol{W}$
\begin{align}
 (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j}  =\frac{b^{w_{i, L-j+1}}}{\sum_{i'=1}^{4}{b^{w_{i', L-j+1}}}}
\end{align}
Therefore,  we have
\begin{align}
 &  (\matrixSymbol{P}(\matrixSymbol{W}, b))_{1, j} :   (\matrixSymbol{P}(\matrixSymbol{W}, b))_{2, j} :   (\matrixSymbol{P}(\matrixSymbol{W}, b))_{3, j} :  (\matrixSymbol{P}(\matrixSymbol{W}, b))_{4, j} \notag \\
= & b^{w_{1, L-j+1}} : b^{w_{2, L-j+1}} : b^{w_{3, L-j+1}} : b^{w_{4, L-j+1}} 
\end{align}

Now that we have $\matrixSymbol{P}(\matrixSymbol{W}^{**}, b) = \matrixSymbol{P}(\matrixSymbol{W}^{*}, b)$, we could get the following result:
\begin{align}
 & b^{w^{**}_{1, L-j+1}} : b^{w^{**}_{2, L-j+1}} : b^{w^{**}_{3, L-j+1}} : b^{w^{**}_{4, L-j+1}}  \notag \\
= & b^{w^{*}_{1, L-j+1}} : b^{w^{*}_{2, L-j+1}} : b^{w^{*}_{3, L-j+1}} : b^{w^{*}_{4, L-j+1}} 
\end{align}

Therefore, $b^{w^{**}_{i, L-j+1}} = T_jb^{w^{*}_{i, L-j+1}}$ for some positive constant $T_j$ that is independent of $\matrixSymbol{W}^{**}$ and $ \matrixSymbol{W}^{*} $. Taking the logarithm with base $b$ on both sides gives ${w^{**}_{i, L-j+1}} = \log_{b}T_j + {w^{*}_{i, L-j+1}}$, which concludes the proof if we set $t_{L-j+1} := \log_{b}T_j$.

\end{description}
\end{proof}

This is useful if one would like to make some predefined PWM (with the restriction that no 0's or 1's are present) work like kernels in a popular type of CNN model where the thresholding-max-linear structure is used with the threshold being 0 (i.e. ReLU activation is used), without changing its capability of regressing/classifying sequences. 

\begin{enumerate}
\item Generally, if we transform a PWM back to the kernel by reversing the transformation described in the main text, we could use this corollary (specificially, the ``only-if'' direction) to assume that the constant difference between convolution and log-likelihood (i.e. $\sum_{j'=1}^{L}{\log_{b} \left( \sum_{i=1}^{4}{(b)^{w_{i, j'}} }\right) }$) is 0 without worrying about changing the underlying PWM; this allows us to skip the reversed normalization step (i.e. $\matrixSymbol{P}(\matrixSymbol{W}, b) \mapsto \matrixSymbol{C}$); the rest two steps (i.e. $\matrixSymbol{C} \mapsto \matrixSymbol{W}', \matrixSymbol{W}' \mapsto \matrixSymbol{W}$ ) are both invertible and could be carried out unambiguously. 
\item Since all elements in $\matrixSymbol{P}(\matrixSymbol{W}, b)$ are within $(0, 1)$, we would end up with a kernel filled with negative elements only. Such negativeness will not be a problem if the CNN model has a linear activation, but it will be if the activation is ReLU: the convolution would be a constant of 0, thus wiping out any potential signals. We could, however, use this corollary (the ``if'' direction) again to avoid this problem without (again) worrying about changing the the underlying PWM: just apply to each of the kernels a positive shift (which could be predefined or be trained) that is big enough to recover the signals of interest from input sequences.
\item In fact, this problem could be completely circumvented, and we could make the kernels classify sequences as good as the original PWMs. Specifically, for each kernel we could make the absolute value of the shift so big that it is impossible to find a sequence whose maximum of convolution by the kernel is negative (which is easy to implement by using as the shift, for example, the absolute value of the smallest element in the kernel); in this case, the ReLU activation behaves like the linear activation precisely and, due to Theorem 2, for the sequence classification problem the shifted kernels under ReLU activation could perform as good as the unshifted kernels perform under linear activation, and thus as good as the original PWMs perform (using log-likelihood).
\end{enumerate}


\section{Selecting  the best PWM for a given kernel by maximum likelihood estimation}

Here we establish a maximum likelihood estimation (MLE) of optimal PWM sets (specifically, the optimal $b$) for CNN models with the popular structure ``input $\rightarrow$ convolution $\rightarrow$ activation $\rightarrow$ global max-pooling $\rightarrow$ arbitrary layers'', where the activation could be either linear or ReLU, or some other function that is possible to preserve prediction performance in Theorem 2. The MLE for CNN models with other structures could be derived in a similar manner.

Since the activation is monotonically increasing, the output of the model will not change if we switch the activation with the global max-pooling layer. In this way, the model becomes ``input $\rightarrow$ convolution $\rightarrow$ global max-pooling $\rightarrow$ activation  $\rightarrow$ arbitrary layers'', and a reasonable (and simple) likelihood could be the joint probability of observing all max-scored fragments given their generative PWMs transformed from all the kernels with base $b$. Of course, if biologically plausible one could argue further (and thus modify the likelihood accordingly) that for a certain kernel, sequences whose convolution by that kernel do not pass the activation should be excluded from the joint probability.


Recall that from Theorem 1 we have

\begin{align}
 (\matrixSymbol{X} \Asterisk \matrixSymbol{W})_j = \log_b{\mathrm{Prob}(\matrixSymbol{X}[1:4, j:(j+L-1)]|\matrixSymbol{P}(\matrixSymbol{W}, b))} + d(\matrixSymbol{W}, b)
\end{align}

where $d(\matrixSymbol{W}, b) = \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{c_{i', j'}} \right)} = \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{b^{w_{i', j'}}} \right)}$.

Now assume that we have $n$ input sequences, $\matrixSymbol{X}^{(1)}, \cdots, \matrixSymbol{X}^{(n)}$, and $k$ kernels, $\matrixSymbol{W}^{(1)}, \cdots, \matrixSymbol{W}^{(k)}$. Define $j^*(s, t) := \arg\max_{j}\mathrm{Prob}(\matrixSymbol{X}^{(s)}[1:4, j:(j+L-1)]|\matrixSymbol{P}(\matrixSymbol{W}^{(t)}, b))$, i.e. the start coordinate of the fragment from $\matrixSymbol{X}^{(s)}$ that is the most possible one to be generated from $\matrixSymbol{P}(\matrixSymbol{W}^{(t)}, b)$. Note that $j^*(s, t)$ does not change when all quantities but $b$ do not change, as $j^*(s, t)$ could be derived from the $b$-independent convolution $\matrixSymbol{X}^{(s)} \Asterisk \matrixSymbol{W}^{(t)}$; therefore, we could treat $j^*(s, t)$ as constants when finding the optimal $b$.

We then could write the following likelihood $q(\matrixSymbol{X}^{(1)}, \cdots, \matrixSymbol{X}^{(n)} | \matrixSymbol{W}^{(1)}, \cdots, \matrixSymbol{W}^{(k)}, b )$:

\begin{align}
 & q(\matrixSymbol{X}^{(1)}, \cdots, \matrixSymbol{X}^{(n)} | \matrixSymbol{W}^{(1)}, \cdots, \matrixSymbol{W}^{(k)}, b )  \notag \\
= & \prod_{s=1}^{n}\prod_{t=1}^{k}\mathrm{Prob}(\matrixSymbol{X}^{(s)}[1:4, j^*(s, t):(j^*(s, t)+L-1)]|\matrixSymbol{P}(\matrixSymbol{W}^{(t)}, b)) \\
= & \prod_{s=1}^{n}\prod_{t=1}^{k}e^{(\ln b) \log_b{\mathrm{Prob}(\matrixSymbol{X}^{(s)}[1:4, j^*(s, t):(j^*(s, t)+L-1)]|\matrixSymbol{P}(\matrixSymbol{W}^{(t)}, b))}} \notag \\
= & \prod_{s=1}^{n}\prod_{t=1}^{k}e^{(\ln b) (\matrixSymbol{X}^{(s)} \Asterisk \matrixSymbol{W}^{(t)})_{j^*(s, t)} - (\ln b) \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i'=1}^{4}{b^{w^{(t)}_{i', j'}} } \right)}} \notag 
\end{align}

Taking the natural logarithm of both sides gives us

\begin{align}
 & \ln q(\matrixSymbol{X}^{(1)}, \cdots, \matrixSymbol{X}^{(n)} | \matrixSymbol{W}^{(1)}, \cdots, \matrixSymbol{W}^{(k)} ) \notag \\
= & \sum_{s=1}^{n}\sum_{t=1}^{k}\left( (\ln b) (\matrixSymbol{X}^{(s)} \Asterisk \matrixSymbol{W}^{(t)})_{j^*(s, t)} - \sum_{j'=1}^{L}{\ln \left( \sum_{i'=1}^{4}{b^{w^{(t)}_{i', j'}} } \right)} \right) \\
= & (\ln b)\sum_{s=1}^{n}\sum_{t=1}^{k}  (\matrixSymbol{X}^{(s)} \Asterisk \matrixSymbol{W}^{(t)})_{j^*(s, t)} - n\sum_{t=1}^{k}\sum_{j'=1}^{L}{\ln \left( \sum_{i'=1}^{4}{e^{(\ln b)w^{(t)}_{i', j'}} } \right)} \notag 
\end{align}

Therefore, we have the following maximum likelihood estimation:

\begin{align}
& \arg\max_bq( \matrixSymbol{X}^{(1)}, \cdots, \matrixSymbol{X}^{(n)} | \matrixSymbol{W}^{(1)}, \cdots, \matrixSymbol{W}^{(k)}, b ) \notag \\
= &  \arg\max_b \ln q(\matrixSymbol{X}^{(1)}, \cdots, \matrixSymbol{X}^{(n)} | \matrixSymbol{W}^{(1)}, \cdots, \matrixSymbol{W}^{(k)}, b ) \\
= & \arg\max_b \left( (\ln b) \sum_{s=1}^{n}\sum_{t=1}^{k} (\matrixSymbol{X}^{(s)} \Asterisk \matrixSymbol{W}^{(t)})_{j^*(s, t)} - n\sum_{t=1}^{k}\sum_{j'=1}^{L}{\ln \left( \sum_{i'=1}^{4}{e^{(\ln b)  w^{(t)}_{i', j'}} } \right)} \right) \notag \\
= & \arg\max_{v:=\ln b}\left( v \sum_{s=1}^{n}\sum_{t=1}^{k}   (\matrixSymbol{X}^{(s)} \Asterisk \matrixSymbol{W}^{(t)})_{j^*(s, t)} - n\sum_{t=1}^{k}\sum_{j'=1}^{L}{\ln \left( \sum_{i'=1}^{4}{e^{v  w^{(t)}_{i', j'}} } \right)} \right) \notag
\end{align}

Although the convolution itself does not involve $b$ (or $v$), the likelihood function has a form of logarithm of sum of exponentials of $b$ (or $v$), which makes it very hard to derive a closed-form solution. In fact, even evaluating the first derivative ($\frac{dq}{db}$ or $\frac{dq}{dv}$) is very cumbersome, and therefore a derivative-free optimizer might be easier to use here.


\section{Comments on the special case where the sequence tensor is padded with zeroes}

Sometimes the input sequences for the CNN model have different lengths. While in theory this is not a problem for training and testing a CNN model with the popular structure ``input $\rightarrow$ convolution $\rightarrow$ activation $\rightarrow$ global max-pooling $\rightarrow$ arbitrary layers'', it does matter in practice: we need an efficient way to package and submit a batch of such sequences to CPU or GPU efficiently to make predictions or calculate the gradients, but generally one can only submit a tensor where all sequences are of equal length. 

A popular solution is to pad zeroes before, after, or around these sequences, such that these padded sequences are of equal length. For example, consider the following two sequences, ``ACGT'' and ``ACGTAT''. Their tensor forms are as follows:

\begin{align}
\text{ACGT: } & \matrixSymbol{X^{(1)}} = \left( \begin{smallmatrix} 
  1 & 0 & 0 & 0 \\  
  0 & 1 & 0 & 0 \\ 
  0 & 0 & 1 & 0 \\ 
  0 & 0 & 0 & 1 \\ 
\end{smallmatrix} \right) \\
\text{ACGTAT: } & \matrixSymbol{X^{(2)}} = \left( \begin{smallmatrix} 
  1 & 0 & 0 & 0 & 1 & 0 \\  
  0 & 1 & 0 & 0 & 0 & 0 \\ 
  0 & 0 & 1 & 0 & 0 & 0 \\ 
  0 & 0 & 0 & 1 & 0 & 1 \\ 
\end{smallmatrix} \right)
\end{align} 

Padding zeroes will thus create the following new tensor for ACGT:

\begin{center}
\begin{tabular}{ c  c  }
  \hline
  Padding method & New tensor for ACGT \\
  \hline			
  Before & $ \matrixSymbol{X^{(1\text{-new})}} = \left( \begin{array}{cccccc} 0 & 0 & 1 & 0 & 0 & 0 \\    0 & 0 & 0 & 1 & 0 & 0 \\   0 & 0 & 0 & 0 & 1 & 0 \\   0 & 0 & 0 & 0 & 0 & 1  \end{array} \right)  $ \\
  After & $ \matrixSymbol{X^{(1\text{-new})}} = \left( \begin{array}{cccccc} 1 & 0 & 0 & 0 & 0 & 0 \\   0 & 1 & 0 & 0 & 0 & 0 \\  0 & 0 & 1 & 0 & 0 & 0 \\   0 & 0 & 0 & 1 & 0 & 0  \end{array} \right)  $ \\
  Around & $ \matrixSymbol{X^{(1\text{-new})}} = \left( \begin{array}{cccccc} 0 & 1 & 0 & 0 & 0 & 0 \\    0 & 0 & 1 & 0 & 0 & 0 \\   0 & 0 & 0 & 1 & 0 & 0 \\   0 & 0 & 0 & 0 & 1 & 0  \end{array} \right)  $ \\
  \hline  
\end{tabular}
\end{center}

Nevertheless, regardless of the way of padding zeroes, we always have the following result:

\begin{theorem}
$\forall b > 1$ and $\forall \matrixSymbol{X}$ with $N$ columns and its zero-padded version $\matrixSymbol{X'} = \left( \matrixSymbol{A^{(1)}}, \matrixSymbol{X}, \matrixSymbol{A^{(2)}}\right)$, where $\matrixSymbol{A^{(1)}}$ and $\matrixSymbol{A^{(2)}}$ are matrices of zeroes (i.e., the padded zeroes) with $m_1$ and $m_2$ columns, respectively, its convolution with a kernel $\matrixSymbol{W}$ of length $L$ has:
\begin{align}
 (\matrixSymbol{X'} \Asterisk \matrixSymbol{W})_j = \log_b{\mathrm{Prob}(\matrixSymbol{X'}[1:4, o:p]|\matrixSymbol{P}(\matrixSymbol{W}, b))} + d'(\matrixSymbol{W}, b, o, p, j)
\end{align}
where $o = \max(j, m_1 + 1)$, $p = \min(j+L-1, m_1 + N)$ and  $d'(\matrixSymbol{W}, b, o, p, j)$ is a ``constant'' that depends on $o, p, j$, and $\matrixSymbol{W}$, $b$.
\end{theorem}

Basically, this theorem states that the calculation of the log-likelihood does not take into account the zero-padded regions (and thus the kernel elements aligned to such regions), even if part of such regions are involved in convolution.


\begin{proof}
The proof is similar to that of Theorem 1 except for some special treatment to the paddings.

From Theorem 1 we have
\begin{align} 
  & \log_{b}{\mathrm{Prob}(\matrixSymbol{X}[1:4, j:(j+L-1)]|\matrixSymbol{P}(\matrixSymbol{W}, b))} \notag \\
= & \sum_{j'=1}^{L}{\log_{b}{\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X}), j'}\right)}}
\end{align}

which also applies to $\matrixSymbol{X'}[1:4, j:(j+L-1)]$ as long as this matrix is also one hot-encoded (i.e., no zero-padding).

For the convolution involving $\matrixSymbol{X'}$, we have:

\begin{align}
  & \left( \matrixSymbol{X'} \Asterisk \matrixSymbol{W} \right)_j \notag \\
%  & \text{(by definition of convolution)} \\
= & \sum_{i=1}^{4}{\sum_{j'=1}^{L}{ \left( x'_{i, j+j'-1}w_{i, L-j'+1} \right) } } \\
  & \text{Replacing $\matrixSymbol{W}$ with its flipped version $\matrixSymbol{W'}$ which has $w'_{i, j} := w_{i, L-j+1}$, we have} \notag \\
= & \sum_{i=1}^{4}{\sum_{j'=1}^{L}{ \left( x'_{i, j+j'-1}w'_{i, j'} \right) }  } \\
  & \text{Noting that all summands outside $\matrixSymbol{X'}[1:4, o:p]$ are completely zero:} \notag \\
= & \sum_{i=1}^{4}{\sum_{j'=o - j + 1}^{p - j + 1}{ \left( x'_{i, j+j'-1}w'_{i, j'} \right) }  } \\
= & \sum_{j'=o - j + 1}^{p - j + 1}{\left( w'_{f(j+j'-1|\matrixSymbol{X'}), j'} \right) } \notag \\
  & \text{Replacing $\matrixSymbol{W'}$ with its exponentiated version $\matrixSymbol{C} := b^{\matrixSymbol{W'}}$, we have} \notag \\
= & \sum_{j'=o - j + 1}^{p - j + 1}{ \log_{b}\left( c_{f(j + j' - 1|\matrixSymbol{X'}), j'} \right) } \\
  & \text{Replacing $\matrixSymbol{C}$ with $\matrixSymbol{P}(\matrixSymbol{W}, b)$ where $ (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j}  := \frac{c_{i, j}}{\sum_{i'=1}^{4}{c_{i', j}}} $,  we have} \notag \\
= & \sum_{j'=o - j + 1}^{p - j + 1}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X'}), j'} \sum_{i'=1}^{4}{c_{i', j'}} \right) } \\
%  & \text{Decomposing the logarithm gives} \\
= & \sum_{j'=o - j + 1}^{p - j + 1}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1 |\matrixSymbol{X'}), j'} \right) } + \sum_{j'=o - j + 1}^{p - j + 1}{\log_{b} \left( \sum_{i'=1}^{4}{c_{i', j'}} \right) } \notag \\
= & \sum_{j'=1}^{p - o + 1}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(o + j' - 1|\matrixSymbol{X'}), j'} \right) } + \sum_{j'=1}^{p - o + 1}{\log_{b} \left( \sum_{i'=1}^{4}{c_{i', j' + o - j}} \right) } \notag \\
  & \text{Inserting the probability formula for PWM derived above gives us} \notag \\ 
= & \log_{b}{\mathrm{Prob}(\matrixSymbol{X'}[1:4, o:p]|\matrixSymbol{P}(\matrixSymbol{W}, b))} + \sum_{j'=1}^{p - o + 1}{\log_{b} \left( \sum_{i'=1}^{4}{c_{i', j' + o - j}} \right) } \\
= & \log_{b}{\mathrm{Prob}(\matrixSymbol{X'}[1:4, o:p]|\matrixSymbol{P}(\matrixSymbol{W}, b))} + d'(\matrixSymbol{W}, b, o, p, j) \notag 
\end{align}

which concludes the proof.


\end{proof}

Note that, although the convolution is still equivalent to the log-likelihood, a direct comparison between log-likelihoods of different alignments now becomes harder, since one must also consider the specific value of $d'$ for each alignment.


\section{Comments on the special case where the sequence tensor is padded with one quarters (0.25)}

Another popular choice of elements to pad is one quarter (0.25). While padding zeroes essentially ignores all information falling into the padded region, padding one quarters assumes a uniform background distribution on such region and computes the ``background'' signal therein.

The example for the sequence ``ACGT'' padded to length 6 is shown below:

\begin{center}
\begin{tabular}{ c  c  }
  \hline
  Padding method & New tensor for ACGT \\
  \hline			
  Before & $ \matrixSymbol{X^{(1\text{-new})}} = \left( \begin{array}{cccccc} 0.25 & 0.25 & 1 & 0 & 0 & 0 \\    0.25 & 0.25 & 0 & 1 & 0 & 0 \\   0.25 & 0.25 & 0 & 0 & 1 & 0 \\   0.25 & 0.25 & 0 & 0 & 0 & 1  \end{array} \right)  $ \\
  After & $ \matrixSymbol{X^{(1\text{-new})}} = \left( \begin{array}{cccccc} 1 & 0 & 0 & 0 & 0.25 & 0.25 \\   0 & 1 & 0 & 0 & 0.25 & 0.25 \\  0 & 0 & 1 & 0 & 0.25 & 0.25 \\   0 & 0 & 0 & 1 & 0.25 & 0.25  \end{array} \right)  $ \\
  Around & $ \matrixSymbol{X^{(1\text{-new})}} = \left( \begin{array}{cccccc} 0.25 & 1 & 0 & 0 & 0 & 0.25 \\    0.25 & 0 & 1 & 0 & 0 & 0.25 \\   0.25 & 0 & 0 & 1 & 0 & 0.25 \\   0.25 & 0 & 0 & 0 & 1 & 0.25  \end{array} \right)  $ \\
  \hline  
\end{tabular}
\end{center}

Similar to the zero-padding, we always have the following result for all ways of padding:

\begin{theorem}
$\forall b > 1$ and $\forall \matrixSymbol{X}$ with $N$ columns and its one quarter-padded version $\matrixSymbol{X''} = \left( \matrixSymbol{A^{(1)}}, \matrixSymbol{X}, \matrixSymbol{A^{(2)}}\right)$, where $\matrixSymbol{A^{(1)}}$ and $\matrixSymbol{A^{(2)}}$ are matrices of one quarters (i.e., the padded one quarters) with $m_1$ and $m_2$ columns, respectively, its convolution with a kernel $\matrixSymbol{W}$ of length $L$ has:
\begin{align}
 (\matrixSymbol{X''} \Asterisk \matrixSymbol{W})_j = 0.25D(\matrixSymbol{P}(\matrixSymbol{W}, b), o, p, j) + \log_b{\mathrm{Prob}(\matrixSymbol{X''}[1:4, o:p]|\matrixSymbol{P}(\matrixSymbol{W}, b))} + d(\matrixSymbol{W}, b)
\end{align}
where $o = \max(j, m_1 + 1)$, $p = \min(j+L-1, m_1 + N)$,  $D(\matrixSymbol{P}(\matrixSymbol{W}, b), o, p, j)$ the log (with base $b$) of multiplication of all probabilities of all 4 types of nucleotides in the range $\{1, \cdots, o-j, p-j+2, \cdots, L\}$ from $(\matrixSymbol{P}(\matrixSymbol{W}, b))$, i.e.,
\begin{align}
D(\matrixSymbol{P}(\matrixSymbol{W}, b), o, p, j) = \sum_{\substack{i \in \{1,2,3,4\} \\ j' \in \{1, \cdots, o-j, p-j+2, \cdots, L\}} }{  \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j'} \right) }
\end{align}
, and $d(\matrixSymbol{W}, b)$ the same constant as in Theorem 1.
\end{theorem}

The basic idea of this theorem is a little different from that for zero-padding; it states that the calculation of the log-likelihood DOES take into account the one quarter-padded regions by multiply 0.25 and all probabilities in such regions together. 

One can also interpret the log's on the RHS as 0.25 $\times$ the logarithm of the joint probability of generating from $\matrixSymbol{P}(\matrixSymbol{W}, b)$ each of the following sequences. If the log-likelihood is defined on these sequences, then the MLE formulae from Section 5 can be used without any further modification:

\begin{enumerate}
\item the original sequnece $\matrixSymbol{X}$ padded with $m_1$ and $m_2$ A's on left and right sides, respectively
\item the original sequnece $\matrixSymbol{X}$ padded with $m_1$ and $m_2$ C's on left and right sides, respectively
\item the original sequnece $\matrixSymbol{X}$ padded with $m_1$ and $m_2$ G's on left and right sides, respectively
\item the original sequnece $\matrixSymbol{X}$ padded with $m_1$ and $m_2$ T's on left and right sides, respectively

\end{enumerate}


\begin{proof}

From Theorem 1 we have
\begin{align} 
  & \log_{b}{\mathrm{Prob}(\matrixSymbol{X}[1:4, j:(j+L-1)]|\matrixSymbol{P}(\matrixSymbol{W}, b))} \notag \\
= & \sum_{j'=1}^{L}{\log_{b}{\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X}), j'}\right)}}
\end{align}

which also applies to $\matrixSymbol{X''}[1:4, j:(j+L-1)]$ as long as this matrix is also one hot-encoded (i.e., no one quarter-padding).

For the convolution involving $\matrixSymbol{X''}$, we have:

\begin{align}
  & \left( \matrixSymbol{X''} \Asterisk \matrixSymbol{W} \right)_j \notag \\
%  & \text{(by definition of convolution)} \\
= & \sum_{i=1}^{4}{\sum_{j'=1}^{L}{ \left( x''_{i, j+j'-1}w_{i, L-j'+1} \right) } } \\
  & \text{Replacing $\matrixSymbol{W}$ with its flipped version $\matrixSymbol{W'}$ which has $w'_{i, j} := w_{i, L-j+1}$, we have} \notag \\
= & \sum_{i=1}^{4}{\sum_{j'=1}^{L}{ \left( x''_{i, j+j'-1}w'_{i, j'} \right) }  } \\
  & \text{Noting that all summands outside $\matrixSymbol{X''}[1:4, o:p]$ are all 0.25:} \notag \\
= & \sum_{i=1}^{4}{\sum_{j'=1}^{o - j}{ \left( 0.25w'_{i, j'} \right) }  } + \sum_{i=1}^{4}{\sum_{j'=o - j + 1}^{p - j + 1}{ \left( x''_{i, j+j'-1}w'_{i, j'} \right) }  } + \sum_{i=1}^{4}{\sum_{j'=p - j + 2}^{L}{ \left( 0.25w'_{i, j'} \right) }  }\\
= &  \sum_{i=1}^{4}{\sum_{j'=1}^{o - j}{ \left( 0.25w'_{i, j'} \right) }  } + \sum_{j'=o - j + 1}^{p - j + 1}{\left( w'_{f(j+j'-1|\matrixSymbol{X''}), j'} \right) } + \sum_{i=1}^{4}{\sum_{j'=p - j + 2}^{L}{ \left( 0.25w'_{i, j'} \right) }  } \notag \\
  & \text{Replacing $\matrixSymbol{W'}$ with its exponentiated version $\matrixSymbol{C} := b^{\matrixSymbol{W'}}$, we have} \notag \\
= & \sum_{i=1}^{4}{\sum_{j'=1}^{o - j}{ \left( 0.25 \log_{b}\left(c_{i, j'}\right) \right) }  } + \sum_{j'=o - j + 1}^{p - j + 1}{ \log_{b}\left( c_{f(j + j' - 1|\matrixSymbol{X''}), j'} \right) } + \sum_{i=1}^{4}{\sum_{j'=p - j + 2}^{L}{ \left(  0.25\log_{b}\left(c_{i, j'}\right) \right) }  }\\
  & \text{Replacing $\matrixSymbol{C}$ with $\matrixSymbol{P}(\matrixSymbol{W}, b)$ where $ (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j}  := \frac{c_{i, j}}{\sum_{i'=1}^{4}{c_{i', j}}} $,  we have} \notag \\
= & \sum_{i=1}^{4}{\sum_{j'=1}^{o - j}{ \left( 0.25 \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i', j'} \sum_{i=1}^{4}{c_{i', j'}}  \right) \right) }  } +  \sum_{j'=o - j + 1}^{p - j + 1}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1|\matrixSymbol{X''}), j'} \sum_{i'=1}^{4}{c_{i', j'}} \right) } \notag \\
 & + \sum_{i=1}^{4}{\sum_{j'=p - j + 2}^{L}{ \left(  0.25\log_{b}\left(  (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j'} \sum_{i'=1}^{4}{c_{i', j'}}   \right) \right) }  } \\
%  & \text{Decomposing the logarithm gives} \\
= & 0.25\sum_{i=1}^{4}{ \left( \sum_{j'=1}^{o - j}{  \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j'} \right)   } + \sum_{j'=p-j+2}^{L}{  \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j'} \right)   } \right) } + \sum_{j'=o - j + 1}^{p - j + 1}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(j + j' - 1 |\matrixSymbol{X''}), j'} \right) } \notag\\
 & + \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i=1}^{4}{c_{i, j'}} \right) }  \\
= & 0.25\sum_{i=1}^{4}{ \left( \sum_{j'=1}^{o - j}{  \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j'} \right)   } + \sum_{j'=p-j+2}^{L}{  \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j'} \right)   } \right) } + \sum_{j'=1}^{p - o + 1}{ \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{f(o + j' - 1 |\matrixSymbol{X''}), j'} \right) } \notag\\
 & + \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i=1}^{4}{c_{i, j'}} \right) }  \\
  & \text{Inserting the probability formula for PWM derived above gives us} \notag \\ 
= & 0.25\sum_{i=1}^{4}{ \left( \sum_{j'=1}^{o - j}{  \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j'} \right)   } + \sum_{j'=p-j+2}^{L}{  \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j'} \right)   } \right) } \notag \\
  & +  \log_{b}{\mathrm{Prob}(\matrixSymbol{X''}[1:4, o:p]|\matrixSymbol{P}(\matrixSymbol{W}, b))} + \sum_{j'=1}^{L}{\log_{b} \left( \sum_{i=1}^{4}{c_{i, j'}} \right) } \\
= & 0.25\left( \sum_{\substack{i \in \{1,2,3,4\} \\ j' \in \{1, \cdots, o-j, p-j+2, \cdots, L\}} }{  \log_{b}\left( (\matrixSymbol{P}(\matrixSymbol{W}, b))_{i, j'} \right) } \right) + \log_{b}{\mathrm{Prob}(\matrixSymbol{X''}[1:4, o:p]|\matrixSymbol{P}(\matrixSymbol{W}, b))} + d(\matrixSymbol{W}, b)  \\
= & 0.25D(\matrixSymbol{P}(\matrixSymbol{W}, b), o, p, j) + \log_{b}{\mathrm{Prob}(\matrixSymbol{X''}[1:4, o:p]|\matrixSymbol{P}(\matrixSymbol{W}, b))} + d(\matrixSymbol{W}, b) 
\end{align}

which concludes the proof.


\end{proof}

Similar to the case of zero-padding, a direct comparison between log-likelihoods of different alignments now becomes harder, since one must also consider the specific value of $D$ for each alignment.


\bibliography{for.0..2.01_kernel_to_PWM}
\bibliographystyle{plain}

\end{document}  
